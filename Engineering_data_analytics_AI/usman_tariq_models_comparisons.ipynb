{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the Different Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Usman Tariq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **About Dataset:**\\\n",
    "> We will use the `diamonds` dataset from the seaborn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Purpose:**\n",
    "> + We will apply the following algorithms to the sample of above mentioned dataset:\n",
    ">   01. Logistic Regression\n",
    ">   02. Random Forest\n",
    ">   03. Decision Tree\n",
    ">   04. Gradient Boosting\n",
    ">   05. Support Vector Machine\n",
    ">   06. K-Nearest Neighbors\n",
    ">   07. Extra Gradient Boosting\n",
    ">   08. Ada Boost\n",
    ">   09. Bagging\n",
    ">   10. CatBoost\n",
    ">   11. Light GBM\n",
    ">   12. XGBoost\n",
    ">   13. Naive Bayes\n",
    "> + We will apply the `Hyperparameter Tuning` and select the best algorithm based on the following classification metrics:\n",
    ">   + Accuracy Score\n",
    ">   + Recall Score\n",
    ">   + Precision Score\n",
    ">   + F1 Score\n",
    "> + We will save the best selected algorithm.\n",
    "> + We will load the saved model and run it on a dummy input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the maximum number of columns to display\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "df_full = sns.load_dataset('diamonds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taking the Sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43219</th>\n",
       "      <td>0.40</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>VVS1</td>\n",
       "      <td>62.6</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1397</td>\n",
       "      <td>4.73</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42377</th>\n",
       "      <td>0.43</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>60.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1304</td>\n",
       "      <td>4.92</td>\n",
       "      <td>4.89</td>\n",
       "      <td>2.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43777</th>\n",
       "      <td>0.53</td>\n",
       "      <td>Good</td>\n",
       "      <td>H</td>\n",
       "      <td>IF</td>\n",
       "      <td>61.2</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1440</td>\n",
       "      <td>5.16</td>\n",
       "      <td>5.27</td>\n",
       "      <td>3.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat      cut color clarity  depth  table  price     x     y     z\n",
       "43219   0.40    Ideal     E    VVS1   62.6   56.0   1397  4.73  4.70  2.95\n",
       "42377   0.43  Premium     E    VVS2   60.8   57.0   1304  4.92  4.89  2.98\n",
       "43777   0.53     Good     H      IF   61.2   65.0   1440  5.16  5.27  3.19"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting the sample of 500 rows randomly.\n",
    "df = df_full.sample(100, random_state=42)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying the Different Models for Classification Problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results of Applied Models:\n",
      "              Model  Mean Accuracy  Mean Precision  Mean Recall  Mean F1  Runtime\n",
      "Logistic Regression       0.558328        0.463136     0.558328 0.482010 00:00.41\n",
      "      Random Forest       0.644605        0.542626     0.644605 0.579116 00:49.66\n",
      "      Decision Tree       0.646275        0.639274     0.646275 0.627872 00:01.68\n",
      "  Gradient Boosting       0.637148        0.566944     0.637148 0.575885 01:15.35\n",
      "                SVM       0.528981        0.389014     0.528981 0.424227 00:00.88\n",
      "                KNN       0.526391        0.462862     0.526391 0.484489 00:01.37\n",
      "     Extra Gradient       0.600091        0.485314     0.600091 0.524105 00:36.12\n",
      "          Ada Boost       0.635769        0.492585     0.635769 0.546737 00:06.94\n",
      "            Bagging       0.671277        0.588779     0.671277 0.619194 00:18.07\n",
      "           CatBoost       0.657655        0.532860     0.657655 0.577128 01:49.80\n",
      "          Light GBM       0.609205        0.492773     0.609205 0.525707 00:06.81\n",
      "            XGBoost       0.686629        0.625459     0.686629 0.633739 01:17.90\n",
      "------------------------------------------\n",
      "\n",
      "Best Selected Model based on F1 Score: XGBoost\n",
      "Best Hyperparameters: {'classifier__gamma': 0.2, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 7, 'classifier__n_estimators': 200}\n",
      "CPU times: total: 12min 2s\n",
      "Wall time: 6min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Assume 'cut' is the target variable\n",
    "X = df.drop('cut', axis=1)\n",
    "y = df['cut']\n",
    "\n",
    "# Apply ordinal encoder to the target variable y.\n",
    "y_order = [['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']]\n",
    "ordinal_encoder = OrdinalEncoder(categories=y_order)\n",
    "y_reshaped = y.values.reshape(-1, 1)\n",
    "y_encoded = ordinal_encoder.fit_transform(y_reshaped)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define transformers for numerical and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['category', 'object']).columns\n",
    "\n",
    "# Your manually defined orders for each categorical feature\n",
    "custom_orders = {\n",
    "    'color': ['D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "    'clarity': ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF']\n",
    "}\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OrdinalEncoder(categories=[custom_orders[feature] for feature in categorical_features]))\n",
    "])\n",
    "\n",
    "# Use ColumnTransformer to apply transformers to different feature types\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define models with hyperparameter grids for tuning\n",
    "models = {\n",
    "    'Logistic Regression': (LogisticRegression(), {'C': [0.001, 0.01, 0.1, 1, 10, 100]}),\n",
    "    'Random Forest': (RandomForestClassifier(), {'n_estimators': [50, 100, 200],\n",
    "                                                 'max_depth': [None, 10, 20, 30],\n",
    "                                                 'min_samples_split': [2, 5, 10],\n",
    "                                                 'min_samples_leaf': [1, 2, 4]}),\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {'max_depth': [None, 10, 20, 30],\n",
    "                                                 'min_samples_split': [2, 5, 10],\n",
    "                                                 'min_samples_leaf': [1, 2, 4]}),\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(), {'n_estimators': [50, 100, 200],\n",
    "                                                         'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "                                                         'max_depth': [3, 5, 7]}),\n",
    "    'SVM': (SVC(), {'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "                                        'kernel': ['linear', 'rbf'],\n",
    "                                        'gamma': ['scale', 'auto']}),\n",
    "    'KNN': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7],\n",
    "                                     'weights': ['uniform', 'distance'],\n",
    "                                     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}),\n",
    "    'Extra Gradient': (ExtraTreesClassifier(), {'n_estimators': [50, 100, 200],\n",
    "                                                         'max_depth': [None, 10, 20, 30],\n",
    "                                                         'min_samples_split': [2, 5, 10],\n",
    "                                                         'min_samples_leaf': [1, 2, 4]}),\n",
    "    'Ada Boost': (AdaBoostClassifier(), {'n_estimators': [50, 100, 200],\n",
    "                                         'learning_rate': [0.001, 0.01, 0.1, 1]}),\n",
    "    'Bagging': (BaggingClassifier(), {'n_estimators': [50, 100, 200],\n",
    "                                       'max_samples': [1.0, 0.8, 0.6],\n",
    "                                       'max_features': [1.0, 0.8, 0.6]}),\n",
    "    'CatBoost': (CatBoostClassifier(verbose=False), {'n_estimators': [50, 100, 200],\n",
    "                                         'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "                                         'depth': [4, 6, 8, 10]}),\n",
    "    'Light GBM': (LGBMClassifier(verbose=-1), {'n_estimators': [50, 100, 200],\n",
    "                                     'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "                                     'max_depth': [3, 5, 7]}),\n",
    "    'XGBoost': (XGBClassifier(), {'n_estimators': [50, 100, 200],\n",
    "                                   'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "                                   'max_depth': [3, 5, 7],\n",
    "                                   'gamma': [0, 0.1, 0.2, 0.3]}),\n",
    "    # 'Naive Bayes': (GaussianNB(), {})\n",
    "}\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results_table = pd.DataFrame(columns=['Model', 'Mean Accuracy', 'Mean Precision', 'Mean Recall', 'Mean F1'])\n",
    "\n",
    "# Evaluate models and select the best based on multiple metrics\n",
    "best_model = None\n",
    "best_composite_score = 0\n",
    "best_model_params = None\n",
    "\n",
    "for model_name, (model, original_param_grid) in models.items():\n",
    "    start_time = time()  # Record the start time\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('classifier', model)])\n",
    "\n",
    "    # Include classifier name in the parameter grid for GridSearchCV\n",
    "    param_grid = {f'classifier__{key}': value for key, value in original_param_grid.items()}\n",
    "\n",
    "    # Define a composite scoring function (you can customize weights based on your preferences)\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': 'precision_weighted',\n",
    "        'recall': 'recall_weighted',\n",
    "        'f1': 'f1_weighted'\n",
    "    }\n",
    "\n",
    "    # Use cross-validation with the composite scoring function\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring=scoring, refit='f1', return_train_score=False)\n",
    "    grid_search.fit(X, y_encoded)\n",
    "\n",
    "    end_time = time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    # Convert run_time to minutes and seconds\n",
    "    minutes, seconds = divmod(runtime, 60)\n",
    "    formatted_time = \"{:0>2}:{:05.2f}\".format(int(minutes), seconds)\n",
    "\n",
    "    # Get the best model and its performance on multiple metrics\n",
    "    best_model_name = model_name\n",
    "    best_model_params = grid_search.best_params_\n",
    "    results = grid_search.cv_results_\n",
    "\n",
    "    # print(\"Well Performed Hyperparameters of\",model_name,\":\\n\",grid_search.best_params_)\n",
    "\n",
    "    # Calculate mean scores for each fold\n",
    "    mean_accuracy = results[\"mean_test_accuracy\"].mean()\n",
    "    mean_precision = results[\"mean_test_precision\"].mean()\n",
    "    mean_recall = results[\"mean_test_recall\"].mean()\n",
    "    mean_f1 = results[\"mean_test_f1\"].mean()\n",
    "\n",
    "    # Instead of results_table = results_table.append(...)\n",
    "    results_table = pd.concat([results_table, pd.DataFrame([{'Model': model_name,\n",
    "                                                          'Mean Accuracy': mean_accuracy,\n",
    "                                                          'Mean Precision': mean_precision,\n",
    "                                                          'Mean Recall': mean_recall,\n",
    "                                                          'Mean F1': mean_f1,\n",
    "                                                          'Runtime': formatted_time}])], ignore_index=True)\n",
    "\n",
    "    # Update the best model if the current model has a better composite score\n",
    "    if mean_f1 > best_composite_score:\n",
    "        best_model = best_model_name\n",
    "        best_composite_score = mean_f1\n",
    "        best_model_params = grid_search.best_params_\n",
    "\n",
    "# Display the results table\n",
    "print('\\nResults of Applied Models:')\n",
    "print(results_table.to_string(index=False, line_width=1000))\n",
    "# print(pd.DataFrame(results_table))\n",
    "\n",
    "print('------------------------------------------')\n",
    "print(f'\\nBest Selected Model based on F1 Score: {best_model}')\n",
    "print(f'Best Hyperparameters: {best_model_params}')\n",
    "\n",
    "\n",
    "\n",
    "# Save the best model using pickle\n",
    "best_model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                       ('classifier', models[best_model][0])])\n",
    "\n",
    "# print(models[best_model][0])\n",
    "\n",
    "best_model_pipeline.fit(X, y_encoded)  # Fit on the entire dataset\n",
    "with open('best_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_model_pipeline, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from the saved file (for testing purpose)\n",
    "with open('best_model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "\n",
    "def prediction(input_data):\n",
    "    prediction = loaded_model.predict(input_data)\n",
    "    if prediction == 0:\n",
    "        print('Fair')\n",
    "    elif prediction == 1:\n",
    "        print('Good')\n",
    "    elif prediction == 2:\n",
    "        print('Very Good')\n",
    "    elif prediction == 3:\n",
    "        print('Premium')\n",
    "    elif prediction == 4:\n",
    "        print('Ideal')\n",
    "    else:\n",
    "        print('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy dataframe for the input variables.\n",
    "dummy_inputs_1 = pd.DataFrame({\n",
    "    'carat': [0.8],\n",
    "    'color': ['G'],\n",
    "    'clarity': ['SI1'],\n",
    "    'depth': [40.5],\n",
    "    'table': [33.0],\n",
    "    'price': [4401],\n",
    "    'x': [4.88],\n",
    "    'y': [4.66],\n",
    "    'z': [2.9]\n",
    "})\n",
    "prediction(dummy_inputs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal\n"
     ]
    }
   ],
   "source": [
    "# Create another dummy dataframe for the input variables.\n",
    "dummy_inputs_2 = pd.DataFrame({\n",
    "    'carat': [1.2],\n",
    "    'color': ['D'],\n",
    "    'clarity': ['VS2'],\n",
    "    'depth': [60.5],\n",
    "    'table': [53.1],\n",
    "    'price': [5401],\n",
    "    'x': [3.88],\n",
    "    'y': [3.66],\n",
    "    'z': [2.9]\n",
    "})\n",
    "prediction(dummy_inputs_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
